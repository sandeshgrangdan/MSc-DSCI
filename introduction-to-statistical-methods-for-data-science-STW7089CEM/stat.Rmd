```{r}
install.packages("matlib")
install.packages("rsample")
```


#importing needed liberary
```{r}
Sys.setenv(RGL_USE_NULL=TRUE)
library(matlib)
library(ggplot2)
library(rsample)
```

# Defining base path variable for the dataset and time
```{r}
dataset_base_path = "~/Documents/msc-dsci/introduction-to-statistical-methods-for-data-science-STW7089CEM/";
```

#importing input data from csv file
```{r}
dataset=as.matrix(read.csv( file = file.path(dataset_base_path, "dataset.csv"), header = FALSE, skip = 1))

#dataset <- dataset[-1, ]
colnames(dataset)<-c("x1","x3","x4","x5","x2")
dataset_clean <- dataset[complete.cases(dataset), ]
X <- na.omit(na.omit(dataset_clean))[, c("x1", "x3", "x4", "x5")]

#X[, "x4"] <- X[, "x4"] - 900

# Z-score Normalization (baseline)
#X <- scale(X)

#  Quantile Normalization (Normal Score Transformation)
normal_score_transform <- function(x) {
  qnorm((rank(x, ties.method = "average") - 0.5) / length(x))
}
#X <- apply(X, 2, normal_score_transform)

density_of_X=density(X)
plot(density_of_X,main = "Density plot of input signal X")


#x3_column <- X[, "x4"]
#displaying imported data
#X
#importing targated data
Y  = na.omit(na.omit(dataset_clean))[, c("x2")]
#Y <- scale(y)
#Y <- apply(Y, 2, normal_score_transform)
#Y <- scale(Y)
#displaying targeted data
#Y
```



#importing time series data data 
```{r}
time <- as.matrix(1:length(Y))
# time
```


#displaying time series data

```{r}
# time
```

#task 1.1
#plotting time series data

```{r}
X.ts<-ts(X,start = min(time),end = max(time),frequency =1)
Y.ts<-ts(Y,start = min(time),end = max(time),frequency =1)
```

#plotting timeseries data of input and target variable
```{r}
plot(X.ts,main = "Time series plot of X Signal", xlab = "Time", ylab = "Input signal")
plot(Y.ts,main = "Time series plot of Y Signal", xlab = "Time", ylab = "Output signal")
```
# task 1.2 : Plotting distribution of each input EEG signal
# Creating a density of all input signal X 
# Creating a Histogram of X signal
# combining Histogram of X signal with density plot

```{r}


#print(head(X_normalized_standard))
density_of_X=density(X)
plot(density_of_X,main = "Density plot of input signal X")

# Creating a Histogram of X signal
hist(X,freq = FALSE,main = "Density")

#combining Histogram of X signal with density plot
hist(X,freq = FALSE,main = "Density")
lines(density_of_X,lwd=2,col="brown")
rug(jitter(X))
```

#histogram and density plot of individual input signal X and output signal y

```{r}
#Creating a density plot of input signal X1 
density_of_X1=density(X[,"x1"])
hist(X[,"x1"],freq = FALSE,main = "Histogram and density plot of x1",xlab = "x1 Signal")
lines(density_of_X1,lwd=2,col="brown")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x1"]))


#Creating a density plot of input signal X2 
density_of_X3=density(X[,"x3"])
hist(X[,"x3"],freq = FALSE,main = "Histogram and density plot of x2",xlab = "x3 Signal")
lines(density_of_X3,lwd=2,col="brown")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x3"]))


#Creating a density plot of input signal X3
density_of_X4=density(X[,"x4"])
hist(X[,"x4"],freq = FALSE,main = "Histogram and density plot of x4",xlab = "x4 Signal")
lines(density_of_X4,lwd=2,col="brown")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x4"]))


#Creating a density plot of input signal X4
density_of_X5=density(X[,"x5"])
hist(X[,"x5"],freq = FALSE,main = "Histogram and density plot of x4",xlab = "x5 Signal")
lines(density_of_X5,lwd=2,col="brown")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x5"]))


#Creating a density plot of output signal y
density_of_y=density(Y)
hist(Y,freq = FALSE,main = "Histogram and density plot of y(x2)",xlab = "y(x2) Signal")
lines(density_of_y,lwd=2,col="brown")
# Add the data-points with noise in the X-axis
rug(jitter(Y))

```

# Task 1.3 creating scatter plots to indeitify correlation
# arrenging plot in a single screen

```{r}
par(mfrow=c(2,2))

# Plotting input signal X1 against output signal Y
plot(X[,"x1"],Y,main = "Correlation betweeen X1 and Y signal", xlab = "X1 signal", ylab = "Output signal y")

# Plotting input signal X2 against output signal Y
plot(X[,"x3"],Y,main = "Correlation betweeen X3 and Y signal", xlab = "X2 signal", ylab = "Output signal y")

# Plotting input signal X3 against output signal Y
plot(X[,"x4"],Y,main = "Correlation betweeen X4 and Y signal", xlab = "X3 signal", ylab = "Output signal y")

# Plotting input signal X4 against output signal Y
plot(X[,"x5"],Y,main = "Correlation betweeen X5 and Y signal", xlab = "X4 signal", ylab = "Output signal y")
```
# Task 2
# Calculating ones for binding the data
```{r}
ones = matrix(1 , length(X)/4,1)
#ones
```
# Task 2.1
# Calculating thetahat of each candidate model and printing value of theta of each model
```{r}
#Binding data from equation of model 1.
X_model1_predicators <- cbind(X[,"x4"],(X[,"x3"])^2)
X_model1__scaled_predicators <- scale(X_model1_predicators)

X_model1<-cbind(ones,X_model1__scaled_predicators)
#X_model1<-cbind(ones,X[,"x4"],X[,"x3"]^2)

#X_model1
#Calculating thetahat of Model 1
Model1_thetahat=solve(t(X_model1) %*% X_model1) %*% t(X_model1) %*% Y
cat("Theta hat for Model 1 ->" , Model1_thetahat, "\n" )
#For model 2
#Binding data from equation of model 2.

X_model2_predicators <- cbind(X[,"x4"],(X[,"x3"])^2,X[,"x5"])
X_model2_scaled_predicators <- scale(X_model2_predicators)

X_model2<-cbind(ones,X_model2_scaled_predicators)
#X_model2<-cbind(ones,X[,"x4"],X[,"x3"]^2,X[,"x5"])
#X_model2
#Calculating thetahat of Model 2
Model2_thetahat=solve(t(X_model2) %*% X_model2) %*% t(X_model2) %*% Y
cat("Theta hat for Model 2 ->" , Model2_thetahat, "\n" )

#Model 3
#Binding data from equation of model 3.
X_model3_predicators <- cbind(X[,"x3"],X[,"x4"], (X[,"x5"])^3)
X_model3_scaled_predicators <- scale(X_model3_predicators)

X_model3<-cbind(ones,X_model3_scaled_predicators)
#X_model3<-cbind(ones,X[,"x3"],X[,"x4"],(X[,"x5"])^3)
#X_model3
#Calculating thetahat of Model 3
Model3_thetahat=solve(t(X_model3) %*% X_model3) %*% t(X_model3) %*% Y
cat("Theta hat for Model 3 ->" , Model3_thetahat, "\n" )

#For model 4
#Binding data from equation of model 3.
X_model4<-cbind(ones,X[,"x4"],(X[,"x3"])^2,(X[,"x5"])^3)
#X_model4
#Calculating thetahat of Model 4
X_model4_predicators <- cbind(X[,"x4"],(X[,"x3"])^2,(X[,"x5"])^3)
X_model4_scaled_predicators <- scale(X_model4_predicators)

X_model4<-cbind(ones,X_model4_scaled_predicators)
#Model4_thetahat=solve(t(X_model4) %*% X_model4) %*% t(X_model4) %*% Y
cat("Theta hat for Model 4 ->" , Model4_thetahat, "\n" )

# for Model 5
#Binding data from equation of model 5.
X_model5_predicators <- cbind(X[,"x4"],(X[,"x1"])^2,(X[,"x3"])^2)
X_model5_scaled_predicators <- scale(X_model5_predicators)

X_model5<-cbind(ones,X_model5_scaled_predicators)
#X_model5<-cbind(ones,(X[,"x4"]),(X[,"x1"])^2,(X[,"x3"])^2)
#X_model5
#Calculating thetahat of model 1
Model5_thetahat=solve(t(X_model5) %*% X_model5) %*% t(X_model5) %*% Y
cat("Theta hat for Model 5 ->" , Model5_thetahat, "\n" )

```
# Task 2.2
#Calculating Y-hat and RSS for each model and printing
```{r}
#Calculating Y-hat and RSS Model 1
Y_hat_model1 = X_model1 %*% Model1_thetahat
#Y_hat_model1
#Calculating RSS
RSS_Model_1=sum((Y-Y_hat_model1)^2)
cat("RSS of model 1 -> ", RSS_Model_1 , "\n")

# Calculating Y-hat and RSS of model 2
Y_hat_model2 = X_model2 %*% Model2_thetahat
#Y_hat_model2
#Calculating RSS
RSS_Model_2=sum((Y-Y_hat_model2)^2)
cat("RSS of model 2 -> ", RSS_Model_2 , "\n")

# Calculating Y-hat and RSS of model 3
Y_hat_model3 = X_model3 %*% Model3_thetahat
#Y_hat_model3
#Calculating RSS
RSS_Model_3=sum((Y-Y_hat_model3)^2)
cat("RSS of model 3 -> ", RSS_Model_3 , "\n")

# Calculating Y-hat and RSS of model 4
Y_hat_model4 = X_model4 %*% Model4_thetahat
#Y_hat_model4
#Calculating RSS
RSS_Model_4=sum((Y-Y_hat_model4)^2)
cat("RSS of model 4 -> ", RSS_Model_4 , "\n")

# Calculating Y-hat and RSS of model 5
Y_hat_model5 = X_model5 %*% Model5_thetahat
#Y_hat_model5
#Calculating RSS
RSS_Model_5=sum((Y-Y_hat_model5)^2)
cat("RSS of model 5 -> ", RSS_Model_5 , "\n")
```
#Task 2.3 Calculating likelihood and Variance of each model
```{r}
N=length(Y)

#Calculating the Variance of Model 1
Variance_model1=RSS_Model_1/(N-1)
#Variance_model1
#Calculating the log-likelihood of Model 1
likehood_Model_1=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model1))-(1/(2*Variance_model1))*RSS_Model_1
#likehood_Model_1
cat("Model1 ::Variance =>",Variance_model1, "::Likehood =>", likehood_Model_1,"\n")


#Calculating Variance and log-likelihood of Model 2
Variance_model2=RSS_Model_2/(N-1)
#Variance_model2
likehood_Model_2=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model2))-(1/(2*Variance_model2))*RSS_Model_2
#likehood_Model_2
cat("Model2 ::Variance =>",Variance_model2, "::Likehood =>", likehood_Model_2,"\n")


#Calculating Variance and log-likelihood of Model 3
Variance_model3=RSS_Model_3/(N-1)
#Variance_model3
likehood_Model_3=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model3))-(1/(2*Variance_model3))*RSS_Model_3
#likehood_Model_3
cat("Model3 ::Variance =>",Variance_model3, "::Likehood =>", likehood_Model_3,"\n")

#Calculating Variance and log-likelihood of Model 4
Variance_model4=RSS_Model_4/(N-1)
#Variance_model4
likehood_Model_4=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model4))-(1/(2*Variance_model4))*RSS_Model_4
#likehood_Model_4
cat("Model4 ::Variance =>",Variance_model4, "::Likehood =>", likehood_Model_4,"\n")

#Calculating Variance and log-likelihood of Model 5
Variance_model5=RSS_Model_5/(N-1)
#Variance_model5
likehood_Model_5=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model5))-(1/(2*Variance_model5))*RSS_Model_5
#likehood_Model_5
cat("Model5 ::Variance =>",Variance_model5, "::Likehood =>", likehood_Model_5,"\n")

```

# Task 2.4 
# Calculating AIC And BIC of each model
```{r}
# Calculating AIC and BIC of model 1
K_model1<-length(Model1_thetahat)
#K_model1
AIC_model1=2*K_model1-2*likehood_Model_1
#AIC_model1
BIC_model1=K_model1*log(N)-2*likehood_Model_1
#BIC_model1
cat("Model1 ::K_Value =>",K_model1, "::AIC =>", AIC_model1, "::BIC", BIC_model1 ,"\n")

## thetahat of model 2
K_model2<-length(Model2_thetahat)
#K_model2
##Calculating AIC and BIC of model 2
AIC_model2=2*K_model2-2*likehood_Model_2
#AIC_model2
BIC_model2=K_model2*log(N)-2*likehood_Model_2
#BIC_model2
cat("Model2 ::K_Value =>",K_model2, "::AIC =>", AIC_model2, "::BIC", BIC_model2 ,"\n")

## thetahat of model 3
K_model3<-length(Model3_thetahat)
#K_model3
##Calculating AIC and BIC of model 3
AIC_model3=2*K_model3-2*likehood_Model_3
#AIC_model3
BIC_model3=K_model3*log(N)-2*likehood_Model_3
#BIC_model3
cat("Model3 ::K_Value =>",K_model3, "::AIC =>", AIC_model3, "::BIC", BIC_model3 ,"\n")

## thetahat of model 4
K_model4<-length(Model4_thetahat)
#K_model4
##Calculating AIC and BIC of model 4
AIC_model4=2*K_model4-2*likehood_Model_4
#AIC_model4
BIC_model4=K_model4*log(N)-2*likehood_Model_4
#BIC_model4
cat("Model4 ::K_Value =>",K_model4, "::AIC =>", AIC_model4, "::BIC", BIC_model4 ,"\n")

## thetahat of model 5
K_model5<-length(Model5_thetahat)
#K_model5
##Calculating AIC and BIC of model 5
AIC_model5=2*K_model5-2*likehood_Model_5
#AIC_model5
BIC_model5=K_model5*log(N)-2*likehood_Model_5
#BIC_model5
cat("Model1 ::K_Value =>",K_model1, "::AIC =>", AIC_model1, "::BIC", BIC_model1 ,"\n")
cat("Model2 ::K_Value =>",K_model2, "::AIC =>", AIC_model2, "::BIC", BIC_model2 ,"\n")
cat("Model3 ::K_Value =>",K_model3, "::AIC =>", AIC_model3, "::BIC", BIC_model3 ,"\n")
cat("Model4 ::K_Value =>",K_model4, "::AIC =>", AIC_model4, "::BIC", BIC_model4 ,"\n")
cat("Model5 ::K_Value =>",K_model5, "::AIC =>", AIC_model5, "::BIC", BIC_model5 ,"\n")

```

## All RSS, Likelihood, AIC, BIC
```{r}
df <- data.frame(
  Name = c("RSS","Likehood", "AIC", "BIC"),
  model1 = c(RSS_Model_1, likehood_Model_1, AIC_model1, BIC_model1),
  model2 = c(RSS_Model_2, likehood_Model_2, AIC_model2, BIC_model2),
  model3 = c(RSS_Model_3, likehood_Model_3, AIC_model3, BIC_model3),
  model4 = c(RSS_Model_4, likehood_Model_4, AIC_model4, BIC_model4),
  model5 = c(RSS_Model_5, likehood_Model_5, AIC_model5, BIC_model5)
)
print(df)

min_rss = min(RSS_Model_1,RSS_Model_2,RSS_Model_3,RSS_Model_4,RSS_Model_5)
max_likehood = max(likehood_Model_1,likehood_Model_2,likehood_Model_3,likehood_Model_4,likehood_Model_5)
min_aic = min(AIC_model1,AIC_model2,AIC_model3,AIC_model4,AIC_model5)
min_bic = min(BIC_model1,BIC_model2,BIC_model3,BIC_model4,BIC_model5)

select_best_model <- function(x) {
  if (x == "1" ||x == RSS_Model_1 || x == likehood_Model_1 || x == AIC_model1 || x == BIC_model1 ) {
    return("model1")
  } else if (x == RSS_Model_2 || x == likehood_Model_2 || x == AIC_model2 || x == BIC_model2 ) {
    return("model2")
  } else if (x == RSS_Model_3 || x == likehood_Model_3 || x == AIC_model3 || x == BIC_model3 ) {
    return("model3")
  } else if (x == RSS_Model_4 || x == likehood_Model_4 || x == AIC_model4 || x == BIC_model4 ) {
    return("model4")
  } else if (x == RSS_Model_5 || x == likehood_Model_5 || x == AIC_model5 || x == BIC_model5 ) {
    return("model5")
  } else {
    return("None")
  }
}

print_betas_value_by_model <- function(model) {
  if (model == "model1") {
    cat("Model =>", model, "::thetahat =>" , Model1_thetahat)
  } else if (model == "model2") {
    cat("Model =>", model, "::thetahat =>" , Model2_thetahat)
  } else if (model == "model3") {
    cat("Model =>", model, "::thetahat =>" , Model3_thetahat)
  } else if (model == "model4") {
    cat("Model =>", model, "::thetahat =>" , Model4_thetahat)
  } else if (model == "model5") {
    cat("Model =>", model, "::thetahat =>" , Model5_thetahat)
  }
}

cat("Min RSS =>", min_rss, ":::Model =>", select_best_model(min_rss) , "\n")
cat("Max Likehood  =>", max_likehood, ":::Model  =>", select_best_model(max_likehood) , "\n")
cat("Min AIC =>", min_aic, ":::Model =>", select_best_model(min_aic) , "\n")
cat("Min BIC =>", min_bic, ":::Model =>", select_best_model(min_bic) , "\n\n")


best_model_data = c(select_best_model(min_rss),select_best_model(max_likehood),select_best_model(min_aic),select_best_model(min_bic))
model_freq_table <- table(best_model_data)
best_model <- names(model_freq_table)[which.max(model_freq_table)]

print_betas_value_by_model(best_model)
```
## Task 2.5 calculating error plotting normal/gaussian distibution of each plot

```{r}
par(mfrow=c(1,1))

## Error of model1
model1_error <- Y-Y_hat_model1
model1_error

## Plotting the graph QQplot and QQ line of model 1
qqnorm(model1_error, col = "darkblue",main = "QQ plot of model 1")
qqline(model1_error, col = "brown",lwd=1)


## Error of model2
model2_error <- Y-Y_hat_model2 # error of model 2
## Plotting QQplot and QQ line of model 2
qqnorm(model2_error, col = "darkblue",main = "QQ plot of model 2")
qqline(model2_error, col = "brown")


## Error of model3
model3_error <- Y- Y_hat_model3
## Plotting QQplot and QQ line of model 3
qqnorm(model3_error, col = "darkblue",main = "QQ plot of model 3")
qqline(model3_error, col = "brown")

## Error of model4
model4_error <- Y-Y_hat_model4
## Plotting QQplot and QQ line of model 4
qqnorm(model4_error, col = "darkblue",main = "QQ plot of model 4")
qqline(model4_error, col = "brown")

## Error of model5
model5_error <- Y- Y_hat_model5
## Plotting QQplot and QQ line of model 5
qqnorm(model5_error, col = "darkblue",main = "QQ plot of model 5")
qqline(model5_error, col = "brown")
```
# Task 2.7 splitting data into training and testing dataset and calculating estamation based on training dataset
#also plotting normal distribution graph of training data

```{r}
# loading data
df <- data.frame(x1 = X[,"x1"], x3 = X[,"x3"], x4 = X[,"x4"], x5 = X[,"x5"], y = Y)


split <- initial_split(df, prop = .7)  # Stratified by y if needed

#n = 100
# 2. Split into training (70%) and testing (30%)
#train_idx <- sample(seq_len(n), size = 0.7 * n)
train <- training(split)
test <- testing(split)
Y_testing_data<-as.matrix(test$y)

# 3. Construct model 5 predictors: x4, x1², x3²
X_train_raw <- cbind(train$x4, train$x1^2, train$x3^2)
X_test_raw  <- cbind(test$x4,  test$x1^2,  test$x3^2)

# 4. Scale predictors using training set
X_train_scaled <- scale(X_train_raw)
scaling <- attributes(X_train_scaled)
X_test_scaled <- scale(X_test_raw,center = scaling$`scaled:center`,scale = scaling$`scaled:scale`)

# 5. Add bias (intercept) term
X_train <- cbind(1, X_train_scaled)
X_test <- cbind(1, X_test_scaled)

# 6. Estimate theta using normal equation
y_train <- train$y
theta_hat <- solve(t(X_train) %*% X_train) %*% t(X_train) %*% y_train
theta_hat

# 7. Predictions on test set
y_pred <- X_test %*% theta_hat

y_hat_len <- 1:length(y_pred)
#length(y_pred)
#y_hat_len

# 8. Estimate variance and compute confidence intervals
y_train_pred <- X_train %*% theta_hat
residuals <- y_train - y_train_pred
n_train <- nrow(X_train)
p <- ncol(X_train)
sigma2 <- sum(residuals^2) / (n_train - p)

# 9. Standard errors and 95% confidence intervals
#se_pred <- sqrt(sigma2 * rowSums((X_test %*% solve(t(X_train) %*% X_train)) * X_test))

# 9. Standard errors and 95% confidence intervals
# Variance-covariance matrix of the coefficients
cov_theta <- sigma2 * solve(t(X_train) %*% X_train)
# Variance of the prediction
var_pred <- rowSums((X_test %*% cov_theta) * X_test)
se_pred <- sqrt(sigma2 + var_pred)
z_95 <- qnorm(0.975)

z_95 <- 1.96
ci_lower <- y_pred - z_95 * se_pred
ci_upper <- y_pred + z_95 * se_pred

# 10. Plot actual vs predicted with error bars

plot(y_pred, y,
     col = "blue", pch = 19,
     xlim = range(c(ci_lower, ci_upper, Y_testing_data)),
     ylab = "Test Sample Index", xlab = "Y_testing_hat(Prediction)",
     main = "Predictions with 95% Confidence Intervals")

# Add horizontal error bars using segments
segments(y0 = y_hat_len, x0 = ci_lower, y1 = y_hat_len, x1 = ci_upper, col = rgb(1, 0, 0, 0.01), lwd = 2, pch = 2)

# Add actual test data points
points(Y_testing_data, y, col = "black", pch = 4)

# Add legend
legend("topright", legend = c("Y_testing_hat(Prediction)", "95% CI", "Y_testing_data(Actual)"),
       col = c("blue", rgb(1, 0, 0, 0.01), "black"), pch = c(19, NA, 4), lty = c(1, NA, NA))

#plot_df <- data.frame(
#  index = seq_along(y_pred),
#  actual = test$y,
#  predicted = y_pred,
#  lower = ci_lower,
#  upper = ci_upper
#)

#ggplot(plot_df, aes(x = index)) +
#  geom_point(aes(y = actual), color = "blue", size = 2, shape = 1) + # Actual data points
#  geom_point(aes(y = predicted), color = "red", size = 2) + # Predicted values
#  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.3, color = "gray", alpha = 0.7) + # Add error bars
#  labs(
#    title = "Test Set Predictions with 95% Confidence Intervals",
#    x = "Test Sample Index",
#    y = "y"
#  ) +
#  theme_minimal() +
#  scale_color_manual(values = c("blue", "red"), labels = c("Actual", "Predicted")) +
#  theme(legend.position = "top")

```
#Task 3
```{r}
## Model5 will be used, parameter are selected and kept constant.
#Model5_thetahat
#values from thetahat
thetebias <- 454.365 #selected parameter
thetaone <- 1.349107 # selected parameter
thetatwo <- -10.60533 # constant value
thetathree <- -5.173124 # constant value

arr_1_r=0
arr_2_r=0
Epison <- RSS_Model_5 * 2 ## fixing value of eplision
num_r <- 100 #number of iteration

##Calculating Y-hat for performing rejection ABC
counter_r <- 0
for (i in 1:num) {
  range1_r <- runif(1,-454.365,454.365) # calculating the range
  range2_r <- runif(1,-10.60533 ,10.60533 )
  New_thetahat_r <- matrix(c(range1_r,thetaone ,range2_r,thetathree))
  New_Y_Hat_r <- X_model5 %*% New_thetahat_r ## calculating new Y-hat
  new_RSS_r <- sum((Y-New_Y_Hat_r)^2)
  if (new_RSS_r > Epison){
    arr_1_r[i] <- range1_r
    arr_2_r[i] <- range2_r
    counter_r = counter+1
    
    f_value_r <- matrix(arr_1_r)
    s_value_r <- matrix(arr_2_r)
  }
}

hist(f_value_r)
hist(s_value_r)

###ploting Joint and Marginal Posterior Distribution of the graph
plot(f_value_r,s_value_r, col = c("brown", "blue"), main = "Joint and Marginal Posterior Distribution")
par(mfrow=c(1,1))

arr_1=0
arr_2=0

f_value=0
s_value=0
num <- 10000
##Calculating Y-hat for performing rejection ABC
counter <- 0
for (i in 1:num) {
  range1 <- runif(1,-454.365,454.365) # calculating the range
  range2 <- runif(1,-10.60533 ,10.60533 )
  New_thetahat <- matrix(c(range1,thetaone ,range2,thetathree))
  New_Y_Hat <- X_model5 %*% New_thetahat ## calculating new Y-hat
  new_RSS <- sum((Y-New_Y_Hat)^2)
  new_RSS
  if (new_RSS < Epison){
    arr_1[i] <- range1
    arr_2[i] <- range2
    counter = counter+1
    
    f_value <- matrix(arr_1)
    s_value <- matrix(arr_2)
  }
}

hist(f_value)
hist(s_value)

###ploting Joint and Marginal Posterior Distribution of the graph
plot(f_value,s_value, col = c("brown", "blue"), main = "Joint and Marginal Posterior Distribution")
par(mfrow=c(1,1))
```


